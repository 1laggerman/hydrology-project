{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: world\n",
      "my_list:\n",
      "  - bar: test2\n",
      "    foo: test\n",
      "  - bar: test4\n",
      "    foo: test3\n",
      "name: foo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "class MyDumper(yaml.Dumper):\n",
    "\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(MyDumper, self).increase_indent(flow, False)\n",
    "\n",
    "foo = {\n",
    "    'name': 'foo',\n",
    "    'my_list': [\n",
    "        {'foo': 'test', 'bar': 'test2'},\n",
    "        {'foo': 'test3', 'bar': 'test4'}],\n",
    "    'hello': 'world',\n",
    "}\n",
    "\n",
    "print(yaml.dump(foo, Dumper=MyDumper, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from models.LSTM import LSTM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from utils import series_to_samples\n",
    "\n",
    "# Initialize an empty array\n",
    "# main_array = np.empty((0, 3))  # Adjust the shape depending on the dimensions of the arrays you're stacking\n",
    "\n",
    "# # Simulating a loop where new data arrays are added\n",
    "# for i in range(5):\n",
    "#     # Generate some sample data (each array has 3 elements)\n",
    "#     new_array = np.array([i, i + 1, i + 2])\n",
    "    \n",
    "#     # Reshape the new_array to make sure it has the same shape as the main_array for stacking\n",
    "#     new_array = new_array.reshape(1, -1)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         print(main_array.shape)\n",
    "#         print(new_array.shape)\n",
    "#     # Stack the new array vertically\n",
    "#     main_array = np.vstack([main_array, new_array])\n",
    "\n",
    "# print(main_array)\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "df = pd.read_csv('code/data/passengers.csv')\n",
    "timeseries = df[[\"Passengers\"]].values.astype('float32')\n",
    "\n",
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.8)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]\n",
    "\n",
    "lookback = 6\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "# X_train, y_train, X_test, y_test = X_train.\n",
    "\n",
    "print(X_train[:2])\n",
    "print(y_train[:2])\n",
    "# print(X_t[0:10])\n",
    "# print(y_train)\n",
    "\n",
    "class AirModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=4, batch_first=True)\n",
    "        self.linear = nn.Linear(50, 1)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = AirModel()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n",
    "\n",
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(timeseries) * np.nan\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(timeseries) * np.nan\n",
    "    test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]\n",
    "# plot\n",
    "plt.plot(timeseries)\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test attribute files for missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from utils import get_files\n",
    "\n",
    "def get_name(path):\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "def check_folder(folder_name):\n",
    "    data_files, _ = get_files(f'data/Caravan/timeseries/csv/{folder_name}')\n",
    "    attribute_files, _ = get_files(f'data/Caravan/attributes/{folder_name}')\n",
    "\n",
    "    data_files_names = list(map(get_name, data_files))\n",
    "\n",
    "    for att_file in attribute_files:\n",
    "        att_file = pd.read_csv(att_file)\n",
    "        att_names = att_file['gauge_id'].to_list()\n",
    "\n",
    "        i = 0\n",
    "        while i < len(att_names):\n",
    "            if att_names[i] != data_files_names[i]:\n",
    "                print(f'mismatch at ({folder_name}, {att_file})')\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "# print(get_name('data/Caravan/timeseries/csv/camelsaus'))\n",
    "files, dirs = get_files('data/Caravan/timeseries/csv')\n",
    "\n",
    "for dir in dirs:\n",
    "    check_folder(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if all files have the same headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_csv_headers(directory, files):\n",
    "    headers = None\n",
    "    all_same = True\n",
    "    mismatched_files = []\n",
    "\n",
    "    # files = os.listdir(directory)\n",
    "    print('checking ', len(files), ' files in ', directory)\n",
    "    \n",
    "    # Loop through all files in the directory\n",
    "    for filename in files:\n",
    "\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Get the headers (column names)\n",
    "                current_headers = list(df.columns)\n",
    "\n",
    "                # If it's the first file, store the headers\n",
    "                if headers is None:\n",
    "                    headers = current_headers\n",
    "                else:\n",
    "                    # Compare headers with the first file\n",
    "                    if headers != current_headers:\n",
    "                        all_same = False\n",
    "                        mismatched_files.append(filename)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    # Report the result\n",
    "    if all_same:\n",
    "        # print(\"All CSV files have the same columns.\")\n",
    "        return headers\n",
    "    else:\n",
    "        # print(\"The following files have mismatched columns:\", mismatched_files)\n",
    "        return None\n",
    "    \n",
    "def check_nans(directory, files):\n",
    "    # Loop through all files in the directory\n",
    "    for filename in files:\n",
    "\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # print('in ', file_path)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            is_nan = df['streamflow'].isna().values\n",
    "            j = 0\n",
    "            for i in range(1, len(is_nan)):\n",
    "                if is_nan[i-1] and not is_nan[i] and j > 0:\n",
    "                    print('NaN break at [', j, ', ', i-1, '] in ', file_path)\n",
    "                    # print('NaNs end at ', i)\n",
    "                elif not is_nan[i-1] and is_nan[i]:\n",
    "                    j = i\n",
    "            # if not all(is_nan[i] >= is_nan[i - 1] for i in range(1, len(is_nan))):\n",
    "            #     print('mixed NaNs in ', file_path)\n",
    "                # return False\n",
    "\n",
    "\n",
    "def check_all_csv_under_root(root, func: callable):\n",
    "    headers = None\n",
    "    all_same = True\n",
    "    mismatched_folders = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root):\n",
    "        if len(files) > 0:\n",
    "            new_headers = func(root, files)\n",
    "            if new_headers is None:\n",
    "                print('folder ', root, ' has mismatched columns')\n",
    "            else:\n",
    "                print('folder ', root, ' has matched columns')\n",
    "                if headers is None:\n",
    "                    headers = new_headers\n",
    "                else:\n",
    "                    if headers != new_headers:\n",
    "                        all_same = False\n",
    "                        mismatched_folders.append(root)\n",
    "\n",
    "    if all_same:\n",
    "        print(\"All CSV files have the same columns.\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./data/Caravan/timeseries/csv/camelsaus\"\n",
    "check_all_csv_under_root(folder_path, check_nans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
